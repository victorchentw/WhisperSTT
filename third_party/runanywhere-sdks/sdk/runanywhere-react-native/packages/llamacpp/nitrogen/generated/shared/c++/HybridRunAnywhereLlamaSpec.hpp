///
/// HybridRunAnywhereLlamaSpec.hpp
/// This file was generated by nitrogen. DO NOT MODIFY THIS FILE.
/// https://github.com/mrousavy/nitro
/// Copyright Â© 2026 Marc Rousavy @ Margelo
///

#pragma once

#if __has_include(<NitroModules/HybridObject.hpp>)
#include <NitroModules/HybridObject.hpp>
#else
#error NitroModules cannot be found! Are you sure you installed NitroModules properly?
#endif



#include <NitroModules/Promise.hpp>
#include <string>
#include <optional>
#include <functional>

namespace margelo::nitro::runanywhere::llama {

  using namespace margelo::nitro;

  /**
   * An abstract base class for `RunAnywhereLlama`
   * Inherit this class to create instances of `HybridRunAnywhereLlamaSpec` in C++.
   * You must explicitly call `HybridObject`'s constructor yourself, because it is virtual.
   * @example
   * ```cpp
   * class HybridRunAnywhereLlama: public HybridRunAnywhereLlamaSpec {
   * public:
   *   HybridRunAnywhereLlama(...): HybridObject(TAG) { ... }
   *   // ...
   * };
   * ```
   */
  class HybridRunAnywhereLlamaSpec: public virtual HybridObject {
    public:
      // Constructor
      explicit HybridRunAnywhereLlamaSpec(): HybridObject(TAG) { }

      // Destructor
      ~HybridRunAnywhereLlamaSpec() override = default;

    public:
      // Properties
      

    public:
      // Methods
      virtual std::shared_ptr<Promise<bool>> registerBackend() = 0;
      virtual std::shared_ptr<Promise<bool>> unregisterBackend() = 0;
      virtual std::shared_ptr<Promise<bool>> isBackendRegistered() = 0;
      virtual std::shared_ptr<Promise<bool>> loadModel(const std::string& path, const std::optional<std::string>& modelId, const std::optional<std::string>& modelName, const std::optional<std::string>& configJson) = 0;
      virtual std::shared_ptr<Promise<bool>> isModelLoaded() = 0;
      virtual std::shared_ptr<Promise<bool>> unloadModel() = 0;
      virtual std::shared_ptr<Promise<std::string>> getModelInfo() = 0;
      virtual std::shared_ptr<Promise<std::string>> generate(const std::string& prompt, const std::optional<std::string>& optionsJson) = 0;
      virtual std::shared_ptr<Promise<std::string>> generateStream(const std::string& prompt, const std::string& optionsJson, const std::function<void(const std::string& /* token */, bool /* isComplete */)>& callback) = 0;
      virtual std::shared_ptr<Promise<bool>> cancelGeneration() = 0;
      virtual std::shared_ptr<Promise<std::string>> generateStructured(const std::string& prompt, const std::string& schema, const std::optional<std::string>& optionsJson) = 0;
      virtual std::shared_ptr<Promise<std::string>> getLastError() = 0;
      virtual std::shared_ptr<Promise<double>> getMemoryUsage() = 0;

    protected:
      // Hybrid Setup
      void loadHybridMethods() override;

    protected:
      // Tag for logging
      static constexpr auto TAG = "RunAnywhereLlama";
  };

} // namespace margelo::nitro::runanywhere::llama
